# Web Scraper Service - GitHub Actions Workflow
#
# This workflow is triggered by a repository_dispatch event from the main app.
# It scrapes the provided URLs and indexes them to Upstash Vector.
#
# Trigger payload:
# {
#   "event_type": "scrape",
#   "client_payload": {
#     "urls_json": "[\"https://example.com\"]",
#     "job_id": "abc123",
#     "brand_slug": "mysite",
#     "callback_url": "https://app.com/api/setup/scraper-callback",
#     // BYOK Vector DB credentials (optional - overrides default Upstash)
#     "vector_db_provider": "upstash",
#     "vector_db_url": "https://custom-index.upstash.io",
#     "vector_db_token": "customer-token",
#     "vector_db_index": "my-index",
#     "vector_db_namespace": "my-namespace"
#   }
# }

name: Web Scraper

on:
  repository_dispatch:
    types: [scrape]

  # Allow manual trigger for testing
  workflow_dispatch:
    inputs:
      urls_json:
        description: "JSON array of URLs to scrape"
        required: true
        default: '["https://example.com"]'
      brand_slug:
        description: "Brand identifier for indexing"
        required: true
        default: "test"
      job_id:
        description: "Job ID for tracking"
        required: false
        default: ""
      engine:
        description: "Scraper engine (cheerio, puppeteer, or firecrawl)"
        required: false
        default: "cheerio"
        type: choice
        options:
          - cheerio
          - puppeteer
          - firecrawl
      total_chunks:
        description: "Total number of chunks for parallel processing"
        required: false
        default: "1"
      chunk_id:
        description: "Current chunk ID (1-based) for this job"
        required: false
        default: "1"

jobs:
  # Prepare the chunk matrix before running scrape jobs
  prepare:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
    steps:
      - name: Determine chunk matrix
        id: set-matrix
        run: |
          # Get total chunks from payload or input (default to 1)
          TOTAL_CHUNKS="${{ github.event.client_payload.total_chunks_count || github.event.inputs.total_chunks || '1' }}"

          # Build JSON array [1] or [1,2,3,...] based on total chunks
          if [ "$TOTAL_CHUNKS" == "1" ]; then
            MATRIX='[1]'
          else
            MATRIX=$(seq 1 $TOTAL_CHUNKS | jq -cs '.')
          fi

          echo "Matrix: $MATRIX"
          echo "matrix=$MATRIX" >> $GITHUB_OUTPUT

  scrape:
    needs: prepare
    runs-on: ubuntu-latest
    timeout-minutes: 360
    strategy:
      fail-fast: false
      matrix:
        chunk: ${{ fromJSON(needs.prepare.outputs.matrix) }}
    name: scrape-chunk-${{ matrix.chunk }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "20"

      - name: Install dependencies (without Puppeteer)
        run: npm install --ignore-optional

      - name: Build TypeScript
        run: npm run build

      # Install Puppeteer only when using puppeteer engine
      - name: Install Puppeteer
        if: github.event.client_payload.engine == 'puppeteer' || github.event.inputs.engine == 'puppeteer'
        run: npm install puppeteer

      # Install Chrome for Puppeteer (if using puppeteer engine)
      - name: Setup Chrome
        if: github.event.client_payload.engine == 'puppeteer' || github.event.inputs.engine == 'puppeteer'
        uses: browser-actions/setup-chrome@latest
        with:
          chrome-version: stable

      - name: Determine URLs for this chunk
        id: get-urls
        env:
          CHUNK_ID: ${{ matrix.chunk }}
          # All chunk data from payload (chunked format)
          CHUNK_1: ${{ github.event.client_payload.chunk_1 || '' }}
          CHUNK_2: ${{ github.event.client_payload.chunk_2 || '' }}
          CHUNK_3: ${{ github.event.client_payload.chunk_3 || '' }}
          CHUNK_4: ${{ github.event.client_payload.chunk_4 || '' }}
          CHUNK_5: ${{ github.event.client_payload.chunk_5 || '' }}
          CHUNK_6: ${{ github.event.client_payload.chunk_6 || '' }}
          CHUNK_7: ${{ github.event.client_payload.chunk_7 || '' }}
          CHUNK_8: ${{ github.event.client_payload.chunk_8 || '' }}
          CHUNK_9: ${{ github.event.client_payload.chunk_9 || '' }}
          CHUNK_10: ${{ github.event.client_payload.chunk_10 || '' }}
          # Legacy format fallback (urls_json in payload)
          LEGACY_URLS: ${{ github.event.client_payload.urls_json || '' }}
          # Fallback for manual triggers
          MANUAL_URLS: ${{ github.event.inputs.urls_json || '' }}
        run: |
          # Select URLs based on chunk ID
          case "$CHUNK_ID" in
            1) URLS="$CHUNK_1" ;;
            2) URLS="$CHUNK_2" ;;
            3) URLS="$CHUNK_3" ;;
            4) URLS="$CHUNK_4" ;;
            5) URLS="$CHUNK_5" ;;
            6) URLS="$CHUNK_6" ;;
            7) URLS="$CHUNK_7" ;;
            8) URLS="$CHUNK_8" ;;
            9) URLS="$CHUNK_9" ;;
            10) URLS="$CHUNK_10" ;;
            *) URLS="" ;;
          esac

          # Fallback to legacy urls_json format from payload
          if [ -z "$URLS" ] && [ -n "$LEGACY_URLS" ]; then
            echo "ðŸ“‹ Using legacy urls_json format from payload"
            URLS="$LEGACY_URLS"
          fi

          # Fallback to manual input (for workflow_dispatch)
          if [ -z "$URLS" ] && [ -n "$MANUAL_URLS" ]; then
            echo "ðŸ“‹ Using manual urls_json input"
            URLS="$MANUAL_URLS"
          fi

          echo "ðŸ“‹ Chunk $CHUNK_ID URLs: ${URLS:0:100}..." 
          echo "urls_json=$URLS" >> $GITHUB_OUTPUT

      - name: Run Scraper
        env:
          # Set memory limit and enable garbage collection (2GB for Cheerio, 4GB for Puppeteer)
          NODE_OPTIONS: ${{ (github.event.client_payload.engine == 'puppeteer' || github.event.inputs.engine == 'puppeteer') && '--max-old-space-size=4096 --expose-gc' || '--max-old-space-size=2048 --expose-gc' }}
          
          # Default Upstash credentials (from repository secrets)
          # These are used when BYOK is not enabled
          UPSTASH_VECTOR_REST_URL: ${{ secrets.UPSTASH_VECTOR_REST_URL }}
          UPSTASH_VECTOR_REST_TOKEN: ${{ secrets.UPSTASH_VECTOR_REST_TOKEN }}
          
          # Redis credentials for job status tracking (always from secrets)
          UPSTASH_REDIS_REST_URL: ${{ secrets.UPSTASH_REDIS_REST_URL }}
          UPSTASH_REDIS_REST_TOKEN: ${{ secrets.UPSTASH_REDIS_REST_TOKEN }}

          # BYOK Vector DB credentials (from dispatch payload - overrides defaults)
          # When these are set, the scraper will use customer's vector DB instead of platform default
          BYOK_VECTOR_DB_PROVIDER: ${{ github.event.client_payload.vector_db_provider || '' }}
          BYOK_VECTOR_DB_URL: ${{ github.event.client_payload.vector_db_url || '' }}
          BYOK_VECTOR_DB_TOKEN: ${{ github.event.client_payload.vector_db_token || '' }}
          BYOK_VECTOR_DB_INDEX: ${{ github.event.client_payload.vector_db_index || '' }}
          BYOK_VECTOR_DB_NAMESPACE: ${{ github.event.client_payload.vector_db_namespace || '' }}

          # Callback configuration
          CALLBACK_SECRET: ${{ secrets.SCRAPER_CALLBACK_SECRET }}

          # Firecrawl API key (for firecrawl engine)
          FIRECRAWL_API_KEY: ${{ secrets.FIRECRAWL_API_KEY }}

          # Job parameters - URLs from previous step
          URLS_JSON: ${{ steps.get-urls.outputs.urls_json }}
          BRAND_SLUG: ${{ github.event.client_payload.brand_slug || github.event.inputs.brand_slug }}
          JOB_ID: ${{ github.event.client_payload.job_id || github.event.inputs.job_id }}
          CALLBACK_URL: ${{ github.event.client_payload.callback_url || '' }}
          SCRAPER_ENGINE: ${{ github.event.client_payload.engine || github.event.inputs.engine || 'cheerio' }}

          # Chunk processing metadata
          CHUNK_ID: ${{ matrix.chunk }}
          TOTAL_CHUNKS: ${{ github.event.client_payload.total_chunks_count || github.event.inputs.total_chunks || 1 }}
        run: |
          echo "ðŸš€ Starting scrape job (Chunk $CHUNK_ID/$TOTAL_CHUNKS)"
          echo "   Brand: $BRAND_SLUG"
          echo "   Job ID: $JOB_ID"
          echo "   Engine: $SCRAPER_ENGINE"
          echo "   Memory limit: $NODE_OPTIONS"
          echo "   URLs count: $(echo "$URLS_JSON" | jq -r 'length' 2>/dev/null || echo 'unknown')"
          
          # Log BYOK status
          if [ -n "$BYOK_VECTOR_DB_URL" ]; then
            echo "   ðŸ” BYOK Mode: Using custom $BYOK_VECTOR_DB_PROVIDER vector DB"
          else
            echo "   ðŸ“¦ Default Mode: Using platform Upstash Vector"
          fi

          npm run scrape

      - name: Job Summary
        if: always()
        run: |
          echo "## Scrape Job Complete (Chunk ${{ env.CHUNK_ID }}/${{ env.TOTAL_CHUNKS }})" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Brand:** ${{ github.event.client_payload.brand_slug || github.event.inputs.brand_slug }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Job ID:** ${{ github.event.client_payload.job_id || github.event.inputs.job_id }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Chunk:** ${{ env.CHUNK_ID }}/${{ env.TOTAL_CHUNKS }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Status:** ${{ job.status }}" >> $GITHUB_STEP_SUMMARY
