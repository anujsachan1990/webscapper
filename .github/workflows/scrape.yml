# Web Scraper Service - GitHub Actions Workflow
#
# This workflow is triggered by a repository_dispatch event from the main app.
# It scrapes the provided URLs and indexes them to Upstash Vector.
#
# Trigger payload:
# {
#   "event_type": "scrape",
#   "client_payload": {
#     "urls_json": "[\"https://example.com\"]",
#     "job_id": "abc123",
#     "brand_slug": "mysite",
#     "callback_url": "https://app.com/api/setup/scraper-callback",
#     // BYOK Vector DB credentials (optional - overrides default Upstash)
#     "vector_db_provider": "upstash",
#     "vector_db_url": "https://custom-index.upstash.io",
#     "vector_db_token": "customer-token",
#     "vector_db_index": "my-index",
#     "vector_db_namespace": "my-namespace"
#   }
# }

name: Web Scraper

on:
  repository_dispatch:
    types: [scrape]

  # Allow manual trigger for testing
  workflow_dispatch:
    inputs:
      urls_json:
        description: "JSON array of URLs to scrape"
        required: true
        default: '["https://example.com"]'
      brand_slug:
        description: "Brand identifier for indexing"
        required: true
        default: "test"
      job_id:
        description: "Job ID for tracking"
        required: false
        default: ""
      engine:
        description: "Scraper engine (cheerio, puppeteer, firecrawl, or firecrawl-docker)"
        required: false
        default: "cheerio"
        type: choice
        options:
          - cheerio
          - puppeteer
          - firecrawl
          - firecrawl-docker
      total_chunks:
        description: "Total number of chunks for parallel processing"
        required: false
        default: "1"
      chunk_id:
        description: "Current chunk ID (1-based) for this job"
        required: false
        default: "1"

jobs:
  # Prepare the chunk matrix before running scrape jobs
  prepare:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
    steps:
      - name: Determine chunk matrix
        id: set-matrix
        run: |
          # Get total chunks from payload or input (default to 1)
          TOTAL_CHUNKS="${{ github.event.client_payload.total_chunks_count || github.event.inputs.total_chunks || '1' }}"

          # Build JSON array [1] or [1,2,3,...] based on total chunks
          if [ "$TOTAL_CHUNKS" == "1" ]; then
            MATRIX='[1]'
          else
            MATRIX=$(seq 1 $TOTAL_CHUNKS | jq -cs '.')
          fi

          echo "Matrix: $MATRIX"
          echo "matrix=$MATRIX" >> $GITHUB_OUTPUT

  scrape:
    needs: prepare
    runs-on: ubuntu-latest
    timeout-minutes: 360
    strategy:
      fail-fast: false
      matrix:
        chunk: ${{ fromJSON(needs.prepare.outputs.matrix) }}
    name: scrape-chunk-${{ matrix.chunk }}

    # Note: Firecrawl Docker is started manually in a step below using the official docker-compose
    # This ensures proper setup of all services including Playwright for JavaScript rendering

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "20"

      - name: Install dependencies (without Puppeteer)
        run: npm install --ignore-optional

      - name: Build TypeScript
        run: npm run build

      # Install Puppeteer only when using puppeteer engine
      - name: Install Puppeteer
        if: github.event.client_payload.engine == 'puppeteer' || github.event.inputs.engine == 'puppeteer'
        run: npm install puppeteer

      # Install Chrome for Puppeteer (if using puppeteer engine)
      - name: Setup Chrome
        if: github.event.client_payload.engine == 'puppeteer' || github.event.inputs.engine == 'puppeteer'
        uses: browser-actions/setup-chrome@latest
        with:
          chrome-version: stable

      # Start Firecrawl Docker using official docker-compose (if using firecrawl-docker engine)
      # This includes the Playwright service for full JavaScript rendering
      - name: Start Firecrawl Docker
        if: github.event.client_payload.engine == 'firecrawl-docker' || github.event.inputs.engine == 'firecrawl-docker'
        run: |
          echo "ðŸ”¥ Setting up Firecrawl with full JavaScript rendering support..."
          
          # Clone the official Firecrawl repository
          echo "ðŸ“¥ Cloning Firecrawl repository..."
          git clone --depth 1 https://github.com/mendableai/firecrawl.git /tmp/firecrawl
          cd /tmp/firecrawl
          
          # Create minimal .env file for self-hosted setup
          echo "ðŸ“ Creating Firecrawl configuration..."
          cat > .env << 'EOF'
          PORT=3002
          HOST=0.0.0.0
          USE_DB_AUTHENTICATION=false
          BULL_AUTH_KEY=firecrawl-local
          LOGGING_LEVEL=info
          NUM_WORKERS_PER_QUEUE=2
          EOF
          # Remove leading whitespace from .env file
          sed -i 's/^[[:space:]]*//' .env
          echo "ðŸ“„ .env file contents:"
          cat .env
          
          # Build and start all Firecrawl services (API, Redis, Playwright)
          echo "ðŸ³ Building and starting Firecrawl services..."
          docker compose build --quiet
          docker compose up -d
          
          echo "â³ Waiting for Firecrawl services to be ready..."
          
          # Wait for all services to be healthy (up to 5 minutes)
          FIRECRAWL_READY=false
          for i in {1..60}; do
            # Check if all containers are running
            RUNNING_CONTAINERS=$(docker compose ps --status running -q | wc -l)
            TOTAL_CONTAINERS=$(docker compose ps -q | wc -l)
            
            echo "   Attempt $i/60 - Containers: $RUNNING_CONTAINERS/$TOTAL_CONTAINERS running"
            
            # Try to hit the scrape endpoint
            HTTP_CODE=$(curl -s -o /tmp/firecrawl_response.json -w "%{http_code}" \
               --max-time 15 \
               http://localhost:3002/v1/scrape -X POST \
               -H "Content-Type: application/json" \
               -d '{"url":"https://example.com","formats":["markdown"]}' 2>/dev/null || echo "000")
            
            if [ "$HTTP_CODE" = "200" ]; then
              echo "âœ… Firecrawl is ready with JavaScript rendering! (HTTP $HTTP_CODE)"
              FIRECRAWL_READY=true
              
              # Verify response has content
              if [ -f /tmp/firecrawl_response.json ]; then
                CONTENT_LENGTH=$(cat /tmp/firecrawl_response.json | jq -r '.data.markdown // ""' | wc -c)
                echo "   Response content length: $CONTENT_LENGTH chars"
              fi
              break
            fi
            
            # Accept 401/402 as service is up (auth mode)
            if [ "$HTTP_CODE" = "401" ] || [ "$HTTP_CODE" = "402" ]; then
              echo "âœ… Firecrawl is responding (HTTP $HTTP_CODE - auth mode)"
              FIRECRAWL_READY=true
              break
            fi
            
            echo "   HTTP response: $HTTP_CODE - waiting..."
            sleep 5
          done
          
          if [ "$FIRECRAWL_READY" = "false" ]; then
            echo "âŒ Firecrawl failed to start properly"
            echo "ðŸ“‹ Container status:"
            docker compose ps
            echo ""
            echo "ðŸ“‹ API container logs:"
            docker compose logs api --tail 100
            echo ""
            echo "ðŸ“‹ Playwright service logs:"
            docker compose logs playwright-service --tail 50 2>/dev/null || echo "No playwright-service logs"
            exit 1
          fi
          
          # Show running services
          echo "ðŸ“‹ Firecrawl services running:"
          docker compose ps
          
          # Test JavaScript rendering capability
          echo ""
          echo "ðŸ§ª Testing JavaScript rendering..."
          JS_TEST=$(curl -s -X POST http://localhost:3002/v1/scrape \
            -H "Content-Type: application/json" \
            -d '{"url":"https://example.com","formats":["markdown"],"waitFor":2000}' 2>/dev/null)
          
          if echo "$JS_TEST" | jq -e '.success == true' > /dev/null 2>&1; then
            echo "âœ… JavaScript rendering is working!"
          else
            echo "âš ï¸ JavaScript rendering test returned unexpected result"
            echo "$JS_TEST" | jq '.' 2>/dev/null || echo "$JS_TEST"
          fi

      - name: Determine URLs for this chunk
        id: get-urls
        env:
          CHUNK_ID: ${{ matrix.chunk }}
          # All chunk data from payload (chunked format)
          CHUNK_1: ${{ github.event.client_payload.chunk_1 || '' }}
          CHUNK_2: ${{ github.event.client_payload.chunk_2 || '' }}
          CHUNK_3: ${{ github.event.client_payload.chunk_3 || '' }}
          CHUNK_4: ${{ github.event.client_payload.chunk_4 || '' }}
          CHUNK_5: ${{ github.event.client_payload.chunk_5 || '' }}
          CHUNK_6: ${{ github.event.client_payload.chunk_6 || '' }}
          CHUNK_7: ${{ github.event.client_payload.chunk_7 || '' }}
          CHUNK_8: ${{ github.event.client_payload.chunk_8 || '' }}
          CHUNK_9: ${{ github.event.client_payload.chunk_9 || '' }}
          CHUNK_10: ${{ github.event.client_payload.chunk_10 || '' }}
          # Legacy format fallback (urls_json in payload)
          LEGACY_URLS: ${{ github.event.client_payload.urls_json || '' }}
          # Fallback for manual triggers
          MANUAL_URLS: ${{ github.event.inputs.urls_json || '' }}
        run: |
          # Select URLs based on chunk ID
          case "$CHUNK_ID" in
            1) URLS="$CHUNK_1" ;;
            2) URLS="$CHUNK_2" ;;
            3) URLS="$CHUNK_3" ;;
            4) URLS="$CHUNK_4" ;;
            5) URLS="$CHUNK_5" ;;
            6) URLS="$CHUNK_6" ;;
            7) URLS="$CHUNK_7" ;;
            8) URLS="$CHUNK_8" ;;
            9) URLS="$CHUNK_9" ;;
            10) URLS="$CHUNK_10" ;;
            *) URLS="" ;;
          esac

          # Fallback to legacy urls_json format from payload
          if [ -z "$URLS" ] && [ -n "$LEGACY_URLS" ]; then
            echo "ðŸ“‹ Using legacy urls_json format from payload"
            URLS="$LEGACY_URLS"
          fi

          # Fallback to manual input (for workflow_dispatch)
          if [ -z "$URLS" ] && [ -n "$MANUAL_URLS" ]; then
            echo "ðŸ“‹ Using manual urls_json input"
            URLS="$MANUAL_URLS"
          fi

          echo "ðŸ“‹ Chunk $CHUNK_ID URLs: ${URLS:0:100}..." 
          echo "urls_json=$URLS" >> $GITHUB_OUTPUT

      - name: Run Scraper
        env:
          # Set memory limit and enable garbage collection
          NODE_OPTIONS: "--max-old-space-size=4096 --expose-gc"
          
          # Default Upstash credentials (from repository secrets)
          UPSTASH_VECTOR_REST_URL: ${{ secrets.UPSTASH_VECTOR_REST_URL }}
          UPSTASH_VECTOR_REST_TOKEN: ${{ secrets.UPSTASH_VECTOR_REST_TOKEN }}
          
          # Redis credentials for job status tracking
          UPSTASH_REDIS_REST_URL: ${{ secrets.UPSTASH_REDIS_REST_URL }}
          UPSTASH_REDIS_REST_TOKEN: ${{ secrets.UPSTASH_REDIS_REST_TOKEN }}

          # BYOK Vector DB credentials (from dispatch payload - overrides defaults)
          BYOK_VECTOR_DB_PROVIDER: ${{ github.event.client_payload.vector_db_provider }}
          BYOK_VECTOR_DB_URL: ${{ github.event.client_payload.vector_db_url }}
          BYOK_VECTOR_DB_TOKEN: ${{ github.event.client_payload.vector_db_token }}
          BYOK_VECTOR_DB_INDEX: ${{ github.event.client_payload.vector_db_index }}
          BYOK_VECTOR_DB_NAMESPACE: ${{ github.event.client_payload.vector_db_namespace }}

          # Callback configuration
          CALLBACK_SECRET: ${{ secrets.SCRAPER_CALLBACK_SECRET }}

          # Firecrawl API key (for firecrawl cloud engine)
          FIRECRAWL_API_KEY: ${{ secrets.FIRECRAWL_API_KEY }}

          # Firecrawl Docker configuration (for firecrawl-docker engine)
          # Uses local container started by docker-compose
          FIRECRAWL_DOCKER_URL: ${{ github.event.client_payload.firecrawl_docker_url || 'http://localhost:3002' }}
          FIRECRAWL_DOCKER_API_KEY: ${{ github.event.client_payload.firecrawl_docker_api_key || '' }}

          # Job parameters
          URLS_JSON: ${{ steps.get-urls.outputs.urls_json }}
          BRAND_SLUG: ${{ github.event.client_payload.brand_slug || github.event.inputs.brand_slug }}
          JOB_ID: ${{ github.event.client_payload.job_id || github.event.inputs.job_id }}
          CALLBACK_URL: ${{ github.event.client_payload.callback_url || '' }}
          SCRAPER_ENGINE: ${{ github.event.client_payload.engine || github.event.inputs.engine || 'cheerio' }}
          
          # Chunk processing metadata
          CHUNK_ID: ${{ matrix.chunk }}
          TOTAL_CHUNKS: ${{ github.event.client_payload.total_chunks_count || github.event.inputs.total_chunks || 1 }}
        run: |
          echo "ðŸš€ Starting scrape job (Chunk $CHUNK_ID/$TOTAL_CHUNKS)"
          echo "   Brand: $BRAND_SLUG"
          echo "   Job ID: $JOB_ID"
          echo "   Engine: $SCRAPER_ENGINE"
          echo "   Firecrawl URL: $FIRECRAWL_DOCKER_URL"
          echo "   Memory limit: $NODE_OPTIONS"
          echo "   URLs count: $(echo "$URLS_JSON" | jq -r 'length' 2>/dev/null || echo 'unknown')"
          
          # Log BYOK status
          if [ -n "$BYOK_VECTOR_DB_URL" ]; then
            echo "   ðŸ” BYOK Mode: Using custom $BYOK_VECTOR_DB_PROVIDER vector DB"
          else
            echo "   ðŸ“¦ Default Mode: Using platform Upstash Vector"
          fi

          npm run scrape

      # Show Firecrawl logs on failure for debugging
      - name: Show Firecrawl Logs on Failure
        if: failure() && (github.event.client_payload.engine == 'firecrawl-docker' || github.event.inputs.engine == 'firecrawl-docker')
        run: |
          echo "ðŸ“‹ Firecrawl container logs (for debugging):"
          cd /tmp/firecrawl 2>/dev/null && docker compose logs --tail 200 || echo "No Firecrawl logs available"

      # Cleanup Firecrawl containers
      - name: Cleanup Firecrawl Docker
        if: always() && (github.event.client_payload.engine == 'firecrawl-docker' || github.event.inputs.engine == 'firecrawl-docker')
        run: |
          echo "ðŸ§¹ Cleaning up Firecrawl containers..."
          cd /tmp/firecrawl 2>/dev/null && docker compose down -v --remove-orphans || echo "No Firecrawl containers to clean up"

      - name: Job Summary
        if: always()
        env:
          CHUNK_ID: ${{ matrix.chunk }}
          TOTAL_CHUNKS: ${{ github.event.client_payload.total_chunks_count || github.event.inputs.total_chunks || 1 }}
        run: |
          echo "## Scrape Job Complete (Chunk $CHUNK_ID/$TOTAL_CHUNKS)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Brand:** ${{ github.event.client_payload.brand_slug || github.event.inputs.brand_slug }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Job ID:** ${{ github.event.client_payload.job_id || github.event.inputs.job_id }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Engine:** ${{ github.event.client_payload.engine || github.event.inputs.engine || 'cheerio' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Chunk:** $CHUNK_ID/$TOTAL_CHUNKS" >> $GITHUB_STEP_SUMMARY
          echo "- **Status:** ${{ job.status }}" >> $GITHUB_STEP_SUMMARY
