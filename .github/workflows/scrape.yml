# Web Scraper Service - GitHub Actions Workflow
#
# This workflow is triggered by a repository_dispatch event from the main app.
# It scrapes the provided URLs and indexes them to Upstash Vector.
#
# Trigger payload:
# {
#   "event_type": "scrape",
#   "client_payload": {
#     "urls_json": "[\"https://example.com\"]",
#     "job_id": "abc123",
#     "brand_slug": "mysite",
#     "callback_url": "https://app.com/api/setup/scraper-callback",
#     // BYOK Vector DB credentials (optional - overrides default Upstash)
#     "vector_db_provider": "upstash",
#     "vector_db_url": "https://custom-index.upstash.io",
#     "vector_db_token": "customer-token",
#     "vector_db_index": "my-index",
#     "vector_db_namespace": "my-namespace"
#   }
# }

name: Web Scraper

on:
  repository_dispatch:
    types: [scrape]

  # Allow manual trigger for testing
  workflow_dispatch:
    inputs:
      urls_json:
        description: "JSON array of URLs to scrape"
        required: true
        default: '["https://example.com"]'
      brand_slug:
        description: "Brand identifier for indexing"
        required: true
        default: "test"
      job_id:
        description: "Job ID for tracking"
        required: false
        default: ""
      engine:
        description: "Scraper engine (cheerio, puppeteer, firecrawl, or firecrawl-docker)"
        required: false
        default: "cheerio"
        type: choice
        options:
          - cheerio
          - puppeteer
          - firecrawl
          - firecrawl-docker
      total_chunks:
        description: "Total number of chunks for parallel processing"
        required: false
        default: "1"
      chunk_id:
        description: "Current chunk ID (1-based) for this job"
        required: false
        default: "1"

jobs:
  # Prepare the chunk matrix before running scrape jobs
  prepare:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
    steps:
      - name: Determine chunk matrix
        id: set-matrix
        run: |
          # Get total chunks from payload or input (default to 1)
          TOTAL_CHUNKS="${{ github.event.client_payload.total_chunks_count || github.event.inputs.total_chunks || '1' }}"

          # Build JSON array [1] or [1,2,3,...] based on total chunks
          if [ "$TOTAL_CHUNKS" == "1" ]; then
            MATRIX='[1]'
          else
            MATRIX=$(seq 1 $TOTAL_CHUNKS | jq -cs '.')
          fi

          echo "Matrix: $MATRIX"
          echo "matrix=$MATRIX" >> $GITHUB_OUTPUT

  scrape:
    needs: prepare
    runs-on: ubuntu-latest
    timeout-minutes: 360
    strategy:
      fail-fast: false
      matrix:
        chunk: ${{ fromJSON(needs.prepare.outputs.matrix) }}
    name: scrape-chunk-${{ matrix.chunk }}

    # Service containers for Firecrawl Docker (only used when engine is firecrawl-docker)
    services:
      # PostgreSQL for Firecrawl
      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_USER: firecrawl
          POSTGRES_PASSWORD: firecrawl
          POSTGRES_DB: firecrawl
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      # Redis for Firecrawl
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      # Playwright microservice for full JavaScript rendering
      playwright:
        image: ghcr.io/nicholasgriffintn/playwright-service:latest
        ports:
          - 3000:3000
        options: >-
          --health-cmd "wget --no-verbose --tries=1 --spider http://localhost:3000/health || exit 1"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "20"

      - name: Install dependencies (without Puppeteer)
        run: npm install --ignore-optional

      - name: Build TypeScript
        run: npm run build

      # Install Puppeteer only when using puppeteer engine
      - name: Install Puppeteer
        if: github.event.client_payload.engine == 'puppeteer' || github.event.inputs.engine == 'puppeteer'
        run: npm install puppeteer

      # Install Chrome for Puppeteer (if using puppeteer engine)
      - name: Setup Chrome
        if: github.event.client_payload.engine == 'puppeteer' || github.event.inputs.engine == 'puppeteer'
        uses: browser-actions/setup-chrome@latest
        with:
          chrome-version: stable

      # Start Firecrawl Docker container (if using firecrawl-docker engine)
      - name: Start Firecrawl Docker
        if: github.event.client_payload.engine == 'firecrawl-docker' || github.event.inputs.engine == 'firecrawl-docker'
        run: |
          echo "ðŸ”¥ Starting Firecrawl Docker container..."
          
          # Wait for Playwright service to be ready first
          echo "â³ Waiting for Playwright service..."
          for i in {1..30}; do
            if curl -s http://localhost:3000/health > /dev/null 2>&1; then
              echo "âœ… Playwright service is ready!"
              break
            fi
            echo "   Attempt $i/30 - Playwright not ready yet..."
            sleep 2
          done
          
          # Pull from GitHub Container Registry (correct image location)
          docker pull ghcr.io/mendableai/firecrawl:latest || {
            echo "âš ï¸ Failed to pull from ghcr.io, trying Docker Hub..."
            docker pull firecrawl/firecrawl:latest || {
              echo "âŒ Could not pull Firecrawl image from any registry"
              echo "â„¹ï¸ Will fall back to Cheerio scraper"
              echo "FIRECRAWL_AVAILABLE=false" >> $GITHUB_ENV
              exit 0
            }
          }
          
          # Determine which image was pulled
          IMAGE_NAME="ghcr.io/mendableai/firecrawl:latest"
          if ! docker images | grep -q "ghcr.io/mendableai/firecrawl"; then
            IMAGE_NAME="firecrawl/firecrawl:latest"
          fi
          
          echo "ðŸ“¦ Using image: $IMAGE_NAME"
          
          # Start Firecrawl container with Playwright enabled for full JS rendering
          docker run -d \
            --name firecrawl \
            --network host \
            -e REDIS_URL=redis://localhost:6379 \
            -e REDIS_RATE_LIMIT_URL=redis://localhost:6379 \
            -e PLAYWRIGHT_MICROSERVICE_URL=http://localhost:3000 \
            -e USE_DB_AUTHENTICATION=false \
            -e PORT=3002 \
            -e HOST=0.0.0.0 \
            -e NUM_WORKERS_PER_QUEUE=2 \
            -e LOGGING_LEVEL=info \
            -e SCRAPING_BEE_API_KEY="" \
            -e BLOCK_MEDIA=false \
            $IMAGE_NAME
          
          echo "â³ Waiting for Firecrawl to be ready..."
          
          # Wait for Firecrawl to be healthy (up to 5 minutes with longer intervals)
          FIRECRAWL_READY=false
          for i in {1..60}; do
            # Check if container is still running
            if ! docker ps | grep -q firecrawl; then
              echo "âŒ Firecrawl container stopped unexpectedly"
              docker logs firecrawl --tail 100
              echo "FIRECRAWL_AVAILABLE=false" >> $GITHUB_ENV
              exit 0  # Don't fail the job, will fallback to Cheerio
            fi
            
            # Try to hit the health/scrape endpoint
            HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" \
               --max-time 10 \
               http://localhost:3002/v1/scrape -X POST \
               -H "Content-Type: application/json" \
               -d '{"url":"https://httpbin.org/html","formats":["markdown"]}' 2>/dev/null || echo "000")
            
            if [ "$HTTP_CODE" = "200" ]; then
              echo "âœ… Firecrawl Docker is ready! (HTTP $HTTP_CODE)"
              FIRECRAWL_READY=true
              echo "FIRECRAWL_AVAILABLE=true" >> $GITHUB_ENV
              break
            fi
            
            # Also accept 401/402 as "service is up but auth required"
            if [ "$HTTP_CODE" = "401" ] || [ "$HTTP_CODE" = "402" ]; then
              echo "âœ… Firecrawl Docker is responding (HTTP $HTTP_CODE - auth mode)"
              FIRECRAWL_READY=true
              echo "FIRECRAWL_AVAILABLE=true" >> $GITHUB_ENV
              break
            fi
            
            echo "   Attempt $i/60 - Firecrawl not ready yet (HTTP $HTTP_CODE)..."
            sleep 5
          done
          
          if [ "$FIRECRAWL_READY" = "false" ]; then
            echo "âš ï¸ Firecrawl failed to start in time, will fall back to Cheerio"
            echo "FIRECRAWL_AVAILABLE=false" >> $GITHUB_ENV
          fi
          
          # Show container logs for debugging
          echo "ðŸ“‹ Firecrawl container logs:"
          docker logs firecrawl --tail 50

      - name: Determine URLs for this chunk
        id: get-urls
        env:
          CHUNK_ID: ${{ matrix.chunk }}
          # All chunk data from payload (chunked format)
          CHUNK_1: ${{ github.event.client_payload.chunk_1 || '' }}
          CHUNK_2: ${{ github.event.client_payload.chunk_2 || '' }}
          CHUNK_3: ${{ github.event.client_payload.chunk_3 || '' }}
          CHUNK_4: ${{ github.event.client_payload.chunk_4 || '' }}
          CHUNK_5: ${{ github.event.client_payload.chunk_5 || '' }}
          CHUNK_6: ${{ github.event.client_payload.chunk_6 || '' }}
          CHUNK_7: ${{ github.event.client_payload.chunk_7 || '' }}
          CHUNK_8: ${{ github.event.client_payload.chunk_8 || '' }}
          CHUNK_9: ${{ github.event.client_payload.chunk_9 || '' }}
          CHUNK_10: ${{ github.event.client_payload.chunk_10 || '' }}
          # Legacy format fallback (urls_json in payload)
          LEGACY_URLS: ${{ github.event.client_payload.urls_json || '' }}
          # Fallback for manual triggers
          MANUAL_URLS: ${{ github.event.inputs.urls_json || '' }}
        run: |
          # Select URLs based on chunk ID
          case "$CHUNK_ID" in
            1) URLS="$CHUNK_1" ;;
            2) URLS="$CHUNK_2" ;;
            3) URLS="$CHUNK_3" ;;
            4) URLS="$CHUNK_4" ;;
            5) URLS="$CHUNK_5" ;;
            6) URLS="$CHUNK_6" ;;
            7) URLS="$CHUNK_7" ;;
            8) URLS="$CHUNK_8" ;;
            9) URLS="$CHUNK_9" ;;
            10) URLS="$CHUNK_10" ;;
            *) URLS="" ;;
          esac

          # Fallback to legacy urls_json format from payload
          if [ -z "$URLS" ] && [ -n "$LEGACY_URLS" ]; then
            echo "ðŸ“‹ Using legacy urls_json format from payload"
            URLS="$LEGACY_URLS"
          fi

          # Fallback to manual input (for workflow_dispatch)
          if [ -z "$URLS" ] && [ -n "$MANUAL_URLS" ]; then
            echo "ðŸ“‹ Using manual urls_json input"
            URLS="$MANUAL_URLS"
          fi

          echo "ðŸ“‹ Chunk $CHUNK_ID URLs: ${URLS:0:100}..." 
          echo "urls_json=$URLS" >> $GITHUB_OUTPUT

      - name: Run Scraper
        env:
          # Set memory limit and enable garbage collection (2GB for Cheerio, 4GB for Puppeteer)
          NODE_OPTIONS: ${{ (github.event.client_payload.engine == 'puppeteer' || github.event.inputs.engine == 'puppeteer') && '--max-old-space-size=4096 --expose-gc' || '--max-old-space-size=2048 --expose-gc' }}
          
          # Default Upstash credentials (from repository secrets)
          # These are used when BYOK is not enabled
          UPSTASH_VECTOR_REST_URL: ${{ secrets.UPSTASH_VECTOR_REST_URL }}
          UPSTASH_VECTOR_REST_TOKEN: ${{ secrets.UPSTASH_VECTOR_REST_TOKEN }}
          
          # Redis credentials for job status tracking (always from secrets)
          UPSTASH_REDIS_REST_URL: ${{ secrets.UPSTASH_REDIS_REST_URL }}
          UPSTASH_REDIS_REST_TOKEN: ${{ secrets.UPSTASH_REDIS_REST_TOKEN }}

          # BYOK Vector DB credentials (from dispatch payload - overrides defaults)
          # When these are set, the scraper will use customer's vector DB instead of platform default
          # Note: We use empty string fallback to avoid undefined values
          BYOK_VECTOR_DB_PROVIDER: ${{ github.event.client_payload.vector_db_provider }}
          BYOK_VECTOR_DB_URL: ${{ github.event.client_payload.vector_db_url }}
          BYOK_VECTOR_DB_TOKEN: ${{ github.event.client_payload.vector_db_token }}
          BYOK_VECTOR_DB_INDEX: ${{ github.event.client_payload.vector_db_index }}
          BYOK_VECTOR_DB_NAMESPACE: ${{ github.event.client_payload.vector_db_namespace }}

          # Callback configuration
          CALLBACK_SECRET: ${{ secrets.SCRAPER_CALLBACK_SECRET }}

          # Firecrawl API key (for firecrawl cloud engine)
          FIRECRAWL_API_KEY: ${{ secrets.FIRECRAWL_API_KEY }}

          # Firecrawl Docker configuration (for firecrawl-docker engine)
          # Uses local container by default, or BYOK URL if provided
          FIRECRAWL_DOCKER_URL: ${{ github.event.client_payload.firecrawl_docker_url || 'http://localhost:3002' }}
          FIRECRAWL_DOCKER_API_KEY: ${{ github.event.client_payload.firecrawl_docker_api_key || '' }}

          # Job parameters - URLs from previous step
          URLS_JSON: ${{ steps.get-urls.outputs.urls_json }}
          BRAND_SLUG: ${{ github.event.client_payload.brand_slug || github.event.inputs.brand_slug }}
          JOB_ID: ${{ github.event.client_payload.job_id || github.event.inputs.job_id }}
          CALLBACK_URL: ${{ github.event.client_payload.callback_url || '' }}
          
          # Chunk processing metadata
          CHUNK_ID: ${{ matrix.chunk }}
          TOTAL_CHUNKS: ${{ github.event.client_payload.total_chunks_count || github.event.inputs.total_chunks || 1 }}
        run: |
          # Determine engine with fallback logic
          REQUESTED_ENGINE="${{ github.event.client_payload.engine || github.event.inputs.engine || 'cheerio' }}"
          
          # If firecrawl-docker was requested but isn't available, fall back to cheerio
          if [ "$REQUESTED_ENGINE" = "firecrawl-docker" ] && [ "$FIRECRAWL_AVAILABLE" = "false" ]; then
            echo "âš ï¸ Firecrawl Docker not available, falling back to Cheerio"
            SCRAPER_ENGINE="cheerio"
          else
            SCRAPER_ENGINE="$REQUESTED_ENGINE"
          fi
          
          export SCRAPER_ENGINE
          
          echo "ðŸš€ Starting scrape job (Chunk $CHUNK_ID/$TOTAL_CHUNKS)"
          echo "   Brand: $BRAND_SLUG"
          echo "   Job ID: $JOB_ID"
          echo "   Requested Engine: $REQUESTED_ENGINE"
          echo "   Actual Engine: $SCRAPER_ENGINE"
          echo "   Firecrawl Available: ${FIRECRAWL_AVAILABLE:-not-set}"
          echo "   Memory limit: $NODE_OPTIONS"
          echo "   URLs count: $(echo "$URLS_JSON" | jq -r 'length' 2>/dev/null || echo 'unknown')"
          
          # Log BYOK status
          if [ -n "$BYOK_VECTOR_DB_URL" ]; then
            echo "   ðŸ” BYOK Mode: Using custom $BYOK_VECTOR_DB_PROVIDER vector DB"
          else
            echo "   ðŸ“¦ Default Mode: Using platform Upstash Vector"
          fi

          npm run scrape

      - name: Job Summary
        if: always()
        run: |
          echo "## Scrape Job Complete (Chunk ${{ env.CHUNK_ID }}/${{ env.TOTAL_CHUNKS }})" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Brand:** ${{ github.event.client_payload.brand_slug || github.event.inputs.brand_slug }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Job ID:** ${{ github.event.client_payload.job_id || github.event.inputs.job_id }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Chunk:** ${{ env.CHUNK_ID }}/${{ env.TOTAL_CHUNKS }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Status:** ${{ job.status }}" >> $GITHUB_STEP_SUMMARY
